{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) T\n",
    "2) T\n",
    "3) T \n",
    "4) T\n",
    "5) T\n",
    "6) F\n",
    "7) T\n",
    "8) F\n",
    "9) T\n",
    "10) T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) High dimensional data can suffer from the curse of dimensionality where the variance is spread across multiple dimensions. Dimensionality reduction techniques capitalize on this to remove the dimensions where variance may be grouped to retain the most important features. For low dimension data, the VCR can help indicate how the variance is distributed across few dimensions and whether the variance is more grouped or spread out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Shallow learning has simple learning mechanisms and no serious learning topology. An example of shallow learning is k-NN or linear regression. Alternatively, mid-level learning has the same level learning mechanism as deep-learning, although its learning topology may not be as complex. Examples are ensemble learning like SVM, gradient boosting and random forest. Shallow level learning techniques tend to have higher reproducibiliy than mid-level learning due to their lower complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Standard Scalar is most useful for when the data approximately follows a Gaussian distribution, while MinMax Scalar assumes no particular distribution of the data. Additionally, SS will scale values centered on 0 from [-1, 1] while MM will scale between [0, 1]. MinMax Scalar is good for when we need nonnegative data bounded between [0, 1] like for neural network inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) I would suggest deep-learning methods because they can be highly effective with data that is complex and high-dimensional. Alongside deep-learning, I would suggest ensemble learning, particulary random forest, since each tree is trained on a seperate and random subset of the data, it can be effective at dealing with noise.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import NMF\n",
    "from scipy.linalg import svd\n",
    "\n",
    "def VCR(data):\n",
    "    U, s, V = np.linalg.svd(data)\n",
    "    vcr = s[0] / np.sum(s)\n",
    "\n",
    "    return vcr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "df = pd.read_csv('HFT_AAPL_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_date_to_timestamp(df, column_name):\n",
    "    df[column_name] = pd.to_datetime(df[column_name]).view('int64') // 10**9\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = convert_date_to_timestamp(df, 'Date')\n",
    "df = df.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Scaler: 0.30369767729637925\n",
      "MinMax Scaler: 0.7182453106677202\n",
      "Raw Data: 0.9999094957205015\n"
     ]
    }
   ],
   "source": [
    "scaler_standard = StandardScaler()\n",
    "X_standard = scaler_standard.fit_transform(df)\n",
    "\n",
    "scaler_minmax = MinMaxScaler()\n",
    "X_minmax = scaler_minmax.fit_transform(df)\n",
    "\n",
    "\n",
    "VCR_standard = VCR(X_standard)\n",
    "print(\"Standard Scaler:\", VCR_standard)\n",
    "\n",
    "\n",
    "VCR_minmax = VCR(X_minmax)\n",
    "print(\"MinMax Scaler:\", VCR_minmax)\n",
    "\n",
    "print(\"Raw Data:\", VCR(df.to_numpy()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SS VCR has the lowest variance, with MM VCR being second and the Raw VCR being the highest. This indicates that SS does the best job at reducing variance across dimensions, although it scales between [-1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def nonnegative_SVD(data):\n",
    "    U, s, Vt = svd(data, full_matrices=False)\n",
    "    \n",
    "    # this ensures that the matrices are nonnegative\n",
    "    U[U < 0] = 0\n",
    "    s[s < 0] = 0\n",
    "    Vt[Vt < 0] = 0\n",
    "    \n",
    "    S = np.diag(s)\n",
    "    \n",
    "    nonnegative_data_approx = np.dot(U, np.dot(S, Vt))\n",
    "    \n",
    "    return U, S, Vt, nonnegative_data_approx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_observations(df):\n",
    "    scaler = MinMaxScaler()\n",
    "    df_minmax = scaler.fit_transform(df)\n",
    "    \n",
    "    U_minmax, S_minmax, Vt_minmax, _ = nonnegative_SVD(df_minmax)\n",
    "    \n",
    "    U_raw, s_raw, V_raw, _ = nonnegative_SVD(df.values)\n",
    "    \n",
    "    importance_minmax = np.dot(U_minmax, np.diag(S_minmax))\n",
    "    importance_rank_minmax = np.argsort(-importance_minmax, axis=0)\n",
    "    \n",
    "    importance_raw = np.dot(U_raw, np.diag(s_raw))\n",
    "    importance_rank_raw = np.argsort(-importance_raw, axis=0)\n",
    "    \n",
    "    return importance_rank_minmax, importance_rank_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nonnegative SVD Raw: 0.9048316789545622\n",
      "Nonnegative SVD MinMax: 0.8783901825290915\n",
      "[3900 1169 1170 ...  271  101  185]\n",
      "[   0  390 3900 ... 5653 1242 1239]\n"
     ]
    }
   ],
   "source": [
    "U, S, Vt, nonnegative_data_approx = nonnegative_SVD(df)\n",
    "\n",
    "U_minmax, s_minmax, V_minmax, nonnegative_data_approx_minmax = nonnegative_SVD(X_minmax)\n",
    "\n",
    "print(\"Nonnegative SVD Raw:\", VCR(nonnegative_data_approx))\n",
    "print(\"Nonnegative SVD MinMax:\", VCR(nonnegative_data_approx_minmax))\n",
    "\n",
    "importance_rank_minmax, importance_rank_raw = rank_observations(df)\n",
    "print(importance_rank_minmax)\n",
    "print(importance_rank_raw)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
